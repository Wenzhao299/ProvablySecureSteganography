import argparse
import csv
import json
import os

import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from config import SUPPORTED_MODEL_NAMES, resolve_model_path


def parse_args():
    parser = argparse.ArgumentParser(description="Calculate PPL for stego text generated by stego_parallel.py")
    parser.add_argument("--model", type=str, default="qwen2.5", choices=SUPPORTED_MODEL_NAMES)
    parser.add_argument("--method", type=str, default="ac")
    parser.add_argument("--dataset", type=str, default="movie")
    parser.add_argument("--results_file", type=str, default="", help="Path to stego file (.jsonl/.csv). Default: results_parallel/{dataset}/{method}.jsonl")
    return parser.parse_args()


def safe_get_id_and_text(row: dict):
    if "id" in row and "text" in row:
        return row["id"], row["text"]

    values = list(row.values())
    if len(values) >= 2:
        return values[0], values[1]

    return None, None


def iter_stego_samples(results_file: str):
    if results_file.endswith(".jsonl"):
        with open(results_file, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    row = json.loads(line)
                except json.JSONDecodeError:
                    continue
                if not isinstance(row, dict):
                    continue
                context_id = row.get("id")
                generated_text = row.get("text")
                if context_id is None or generated_text is None:
                    continue
                yield str(context_id), str(generated_text)
        return

    with open(results_file, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            context_id, generated_text = safe_get_id_and_text(row)
            if context_id is None or generated_text is None:
                continue
            yield context_id, generated_text


def main():
    args = parse_args()

    model_path = resolve_model_path(args.model)

    results_file = args.results_file or os.path.join("results_parallel", args.dataset, f"{args.method}.jsonl")
    output_dir = os.path.join("results_parallel", args.dataset, "ppl")
    output_file = os.path.join(output_dir, f"{args.method}.jsonl")
    stats_file = os.path.join(output_dir, f"stats_{args.method}.jsonl")
    os.makedirs(output_dir, exist_ok=True)

    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    if torch.cuda.is_available():
        torch.cuda.set_device(0)

    print(f"Loading model: {model_path} on {device}")
    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True).to(device)
    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
    model.resize_token_embeddings(len(tokenizer))
    model.eval()

    if not os.path.exists(results_file):
        raise FileNotFoundError(f"Stego file not found: {results_file}")

    all_ppls = []

    print(f"Reading stego samples from: {results_file}")
    print(f"Saving PPL results to: {output_file}")

    with open(output_file, "w", encoding="utf-8") as f_out:
        for context_id, generated_text in tqdm(iter_stego_samples(results_file), desc="Calculating PPL"):
            try:
                encodings = tokenizer(generated_text, add_special_tokens=False, return_tensors="pt")
                input_ids = encodings.input_ids.to(device)

                # Need at least 2 tokens to compute shifted language-model loss.
                if input_ids.size(1) < 2:
                    continue

                with torch.no_grad():
                    logits = model(input_ids).logits

                shift_logits = logits[..., :-1, :].contiguous()
                shift_labels = input_ids[..., 1:].contiguous()

                loss_fct = torch.nn.CrossEntropyLoss()
                mean_loss = loss_fct(
                    shift_logits.view(-1, shift_logits.size(-1)),
                    shift_labels.view(-1),
                )

                if torch.isfinite(mean_loss):
                    ppl = torch.exp(mean_loss).item()
                    all_ppls.append(ppl)
                    f_out.write(
                        json.dumps(
                            {"id": str(context_id), "text": generated_text, "ppl": float(ppl)},
                            ensure_ascii=False,
                        )
                        + "\n"
                    )

            except Exception as exc:
                print(f"Skipping id={context_id} due to error: {exc}")

    print(f"PPL calculation complete: {output_file}")

    if not all_ppls:
        print("No valid PPL values were produced.")
        return

    ppl_tensor = torch.tensor(all_ppls)
    stats = {
        "model": args.model,
        "method": args.method,
        "dataset": args.dataset,
        "results_file": results_file,
        "num_samples": len(all_ppls),
        "average_ppl": float(ppl_tensor.mean().item()),
        "max_ppl": float(ppl_tensor.max().item()),
        "min_ppl": float(ppl_tensor.min().item()),
        "std_ppl": float(ppl_tensor.std(unbiased=False).item()),
        "var_ppl": float(ppl_tensor.var(unbiased=False).item()),
    }

    with open(stats_file, "w", encoding="utf-8") as f_stats:
        f_stats.write(json.dumps(stats, ensure_ascii=False) + "\n")

    print("--- PPL Statistics ---")
    print(f"Average PPL: {stats['average_ppl']:.4f}")
    print(f"Maximum PPL: {stats['max_ppl']:.4f}")
    print(f"Minimum PPL: {stats['min_ppl']:.4f}")
    print(f"Standard Deviation: {stats['std_ppl']:.4f}")
    print(f"Variance: {stats['var_ppl']:.4f}")
    print(f"Statistics saved to: {stats_file}")


if __name__ == "__main__":
    main()
